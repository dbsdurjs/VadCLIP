import os
import torch
import multiprocessing
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from torchvision import transforms
from transformers import AutoProcessor, AutoModelForImageTextToText
from tqdm import tqdm

# âœ… CPU ì½”ì–´ ê°œìˆ˜ í™•ì¸ í›„ ì ì ˆí•œ num_workers ì„¤ì •
NUM_WORKERS = multiprocessing.cpu_count()  # CPU ì½”ì–´ ì ˆë°˜ ì‚¬ìš©
print(f"ğŸ”¹ Using num_workers={NUM_WORKERS}")

# ğŸ“Œ 1. ê°œë³„ í”„ë ˆì„ì„ ë¡œë”©í•˜ëŠ” ë°ì´í„°ì…‹ (ê° í´ë˜ìŠ¤ì˜ ì• ì ˆë°˜ë§Œ ì„ íƒ)
class FrameDataset(Dataset):
    def __init__(self, base_folder):
        self.data = []
        self.video_names = []
        self.transform = transforms.Compose([
            # transforms.Resize((224, 224)),
            transforms.Lambda(lambda img: img.convert("RGB")),  # âœ… ë©€í‹°ìŠ¤ë ˆë”© ìµœì í™”
            transforms.ToTensor(),
        ])

        for class_name in os.listdir(base_folder):
            classes_names = os.listdir(base_folder)
            classes_names = classes_names[:8]   # 8ê°œ ì‘ì—…, ì„œë²„ì—ì„œ ë°”ê¾¸ê¸°
            print(f'ì‘ì—… í´ë” ì´ë¦„ : {classes_names}')

            if class_name not in classes_names:
                continue
            
            class_path = os.path.join(base_folder, class_name)
            if not os.path.isdir(class_path):
                continue

            # ğŸ”¹ í´ë˜ìŠ¤ í´ë” ë‚´ë¶€ì˜ ëª¨ë“  ë™ì˜ìƒ í´ë” ê°€ì ¸ì˜¤ê¸°
            # video_folders = sorted(os.listdir(class_path))[:10]
            video_folders = sorted(os.listdir(class_path))
            for video_folder in video_folders:
                video_folder_path = os.path.join(class_path, video_folder)
                if not os.path.isdir(video_folder_path):
                    continue

                # ğŸ“Œ ì„ íƒëœ ë™ì˜ìƒ í´ë”ì˜ í”„ë ˆì„ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
                image_files = sorted([
                    os.path.join(video_folder_path, f) for f in os.listdir(video_folder_path)
                    if f.lower().endswith('.jpg')
                ])
                
                for img in image_files:
                    self.data.append((video_folder_path, img))
                    self.video_names.append(video_folder)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        video_folder_path, image_path = self.data[index]
        video_name = self.video_names[index]

        # âœ… `PIL` ëŒ€ì‹  `transforms.Lambda()` ì‚¬ìš©í•˜ì—¬ ë©€í‹°ìŠ¤ë ˆë”© í™œìš©
        image = Image.open(image_path)
        image = self.transform(image)
        
        return video_folder_path, image_path, image, video_name

# ğŸ“Œ 2. ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ìƒì„±
# base_folder = "/home/yeogeon/YG_main/diffusion_model/VAD_dataset/UCF-Crimes/UCF_Crimes/Extracted_Frames/"
base_folder= "/media/vcl/DATA/YG/Extracted_Frames/"

dataset = FrameDataset(base_folder)
dataloader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)

# ğŸ“Œ 3. ëª¨ë¸ ë¡œë“œ
# processor = AutoProcessor.from_pretrained("microsoft/git-large-coco")
# model = AutoModelForImageTextToText.from_pretrained("microsoft/git-large-coco", torch_dtype=torch.float16)

processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-6.7b")
model = AutoModelForImageTextToText.from_pretrained("Salesforce/blip2-opt-6.7b", torch_dtype=torch.float16)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# ğŸ“Œ 4. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìº¡ì…˜ ìƒì„± (ë¹„ë””ì˜¤ëª… í¬í•¨)
def generate_captions(dataloader, model, processor, device):
    model.eval()
    current_video = None

    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc="Processing Videos", unit="batch", dynamic_ncols=True)

        for batch in progress_bar:
            video_folder_paths, image_paths, images, video_names = batch

            if current_video is None or current_video != video_names[0]:
                current_video = video_names[0]
                print(f"\nğŸ¥ Processing video: {current_video}")

            pixel_values = images.to(device, torch.float16)

            generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
            captions = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

            for video_folder_path, image_path, caption in zip(video_folder_paths, image_paths, captions):
                frame_number = os.path.basename(image_path)
                output_file = os.path.join(video_folder_path, f"{os.path.basename(video_folder_path)}.txt")

                with open(output_file, "a", encoding="utf-8") as f:
                    f.write(f"{frame_number}: {caption}\n")

                print(f'ğŸ–¼ï¸ Frame: {frame_number} | ğŸ“œ Caption: {caption}')

            progress_bar.set_postfix({"Current Video": current_video, "Processed Frames": len(images)})

        print("\nâœ… All videos processed successfully!")

# ğŸ“Œ 4. ê¸°ì¡´ íŒŒì¼ ì‚­ì œ (ì´ì „ ê²°ê³¼ ì§€ìš°ê¸°)
def delete_existing_files(base_folder):
    for class_name in os.listdir(base_folder):
        class_path = os.path.join(base_folder, class_name)
        if not os.path.isdir(class_path):
            continue

        for video_folder in os.listdir(class_path):
            video_folder_path = os.path.join(class_path, video_folder)
            if not os.path.isdir(video_folder_path):
                continue

            output_file = os.path.join(video_folder_path, f"{video_folder}.txt")

            # âœ… ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
            if os.path.exists(output_file):
                os.remove(output_file)
                print(f"ğŸ—‘ï¸ Deleted existing file: {output_file}")

# ğŸ“Œ 5. ìº¡ì…˜ ìƒì„± ì‹¤í–‰
delete_existing_files(base_folder)
generate_captions(dataloader, model, processor, device)
